{"cells":[{"cell_type":"code","execution_count":181,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"/Users/kchu/Documents/Projects/Senior Project\n"}],"source":["# Change directory to VSCode workspace root so that relative path loads work correctly. Turn this addition off with the DataScience.changeDirOnImportExport setting\n","# ms-python.python added\n","import os\n","try:\n","\tos.chdir(os.path.join(os.getcwd(), '..'))\n","\tprint(os.getcwd())\n","except:\n","\tpass\n"]},{"cell_type":"code","execution_count":182,"metadata":{},"outputs":[],"source":["import sys\n","sys.path.insert(0, '..')\n","sys.path.insert(0, '/Users/kchu/Documents/Projects/Senior Project/Claim Extraction/detecting-scientific-claim-master/')\n","\n","from typing import Iterator, List, Dict, Optional\n","import os\n","import json\n","import numpy as np\n","import pandas as pd\n","from itertools import chain\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import precision_recall_fscore_support\n","from allennlp.common.util import JsonDict\n","\n","import torch\n","import torch.optim as optim\n","from torch.nn import ModuleList\n","import torch.nn.functional as F\n","\n","from allennlp.models.archival import load_archive\n","from allennlp.predictors import Predictor\n","from allennlp.common import Params\n","from allennlp.common.file_utils import cached_path\n","\n","from allennlp.data.fields import Field, TextField, LabelField, ListField, SequenceLabelField\n","from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n","from allennlp.data.tokenizers import Token, Tokenizer, WordTokenizer\n","from allennlp.data.token_indexers import PretrainedBertIndexer\n","\n","from allennlp.data.vocabulary import Vocabulary\n","from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n","from allennlp.modules.token_embedders.bert_token_embedder import PretrainedBertEmbedder\n","from allennlp.models import Model\n","from allennlp.nn import InitializerApplicator, RegularizerApplicator\n","from allennlp.training.metrics import CategoricalAccuracy\n","from allennlp.modules.seq2vec_encoders import PytorchSeq2VecWrapper\n","import torch.nn as nn\n","\n","from allennlp.data import Instance\n","from allennlp.data.dataset_readers import DatasetReader\n","from allennlp.data.iterators import BucketIterator, BasicIterator\n","from allennlp.training.trainer import Trainer\n","from allennlp.training.learning_rate_schedulers import LearningRateScheduler\n","\n","from allennlp.modules import Seq2VecEncoder, TimeDistributed, TextFieldEmbedder, ConditionalRandomField, FeedForward\n","from torch.nn.modules.linear import Linear\n","\n","np.random.seed(42)\n","\n","EMBEDDING_DIM = 300\n","TRAIN_PATH = 'https://s3-us-west-2.amazonaws.com/pubmed-rct/train_labels.json'\n","VALIDATION_PATH = 'https://s3-us-west-2.amazonaws.com/pubmed-rct/validation_labels.json'\n","TEST_PATH = 'https://s3-us-west-2.amazonaws.com/pubmed-rct/test_labels.json'\n","# DISCOURSE_MODEL_PATH = './output_crf_pubmed_rct_glove/model.tar.gz'\n","# archive = load_archive(DISCOURSE_MODEL_PATH)\n","# discourse_predictor = Predictor.from_archive(archive, 'discourse_crf_predictor')\n"]},{"cell_type":"code","execution_count":183,"metadata":{},"outputs":[],"source":["class ClaimAnnotationReaderJSON(DatasetReader):\n","    \"\"\"\n","    Reading annotation dataset in the following JSON format:\n","\n","    {\n","        \"paper_id\": ..., \n","        \"user_id\": ...,\n","        \"sentences\": [..., ..., ...],\n","        \"labels\": [..., ..., ...] \n","    }\n","    \"\"\"\n","    def __init__(self,\n","                 tokenizer: Tokenizer = None,\n","                 token_indexers: Dict[str, TokenIndexer] = None,\n","                 lazy: bool = False) -> None:\n","        super().__init__(lazy)\n","        self._tokenizer = tokenizer or WordTokenizer()\n","        self._token_indexers = token_indexers or {'tokens': SingleIdTokenIndexer()}\n","\n","    # @overrides\n","    def _read(self, file_path):\n","        file_path = cached_path(file_path)\n","        with open(file_path, 'r') as file:\n","            for line in file:\n","                example = json.loads(line)\n","                sents = example['sentences']\n","                labels = example['labels']\n","                yield self.text_to_instance(sents, labels)\n","\n","    # @overrides\n","    def text_to_instance(self,\n","                         sents: List[str],\n","                         labels: List[str] = None) -> Instance:\n","        fields: Dict[str, Field] = {}\n","        tokenized_sents = [self._tokenizer.tokenize(sent) for sent in sents]\n","        sentence_sequence = ListField([TextField(tk, self._token_indexers) for tk in tokenized_sents])\n","        fields['sentences'] = sentence_sequence\n","        \n","        if labels is not None:\n","            fields['labels'] = SequenceLabelField(labels, sentence_sequence)\n","        return Instance(fields)\n"]},{"cell_type":"code","execution_count":184,"metadata":{},"outputs":[],"source":["token_indexer = PretrainedBertIndexer(\n","    pretrained_model=\"bert-base-uncased\",\n","    max_pieces=100,\n","    do_lowercase=True,\n"," )\n"]},{"cell_type":"code","execution_count":185,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":"750it [00:07, 104.48it/s]\n375it [00:02, 157.62it/s]\n375it [00:02, 159.44it/s]\n"}],"source":["reader = ClaimAnnotationReaderJSON(\n","    token_indexers={\"tokens\": token_indexer}\n",")\n","\n","train_dataset = reader.read(TRAIN_PATH)\n","validation_dataset = reader.read(VALIDATION_PATH)\n","test_dataset = reader.read(TEST_PATH)"]},{"cell_type":"code","execution_count":186,"metadata":{},"outputs":[],"source":["vocab = Vocabulary()\n","\n","vocab._token_to_index['labels'] = {'0': 0, '1': 1}\n"]},{"cell_type":"code","execution_count":187,"metadata":{},"outputs":[],"source":["\"\"\"Prepare iterator\"\"\"\n","from allennlp.data.iterators import BasicIterator\n","\n","iterator = BasicIterator(batch_size=64)\n","\n","iterator.index_with(vocab)\n"]},{"cell_type":"code","execution_count":188,"metadata":{},"outputs":[],"source":["def multiple_target_CrossEntropyLoss(logits, labels):\n","    loss = 0\n","    for i in range(logits.shape[0]):\n","        loss = loss + nn.CrossEntropyLoss()(logits[i, :, :], labels[i, :])\n","    return loss\n"]},{"cell_type":"code","execution_count":189,"metadata":{},"outputs":[],"source":["\"\"\"Prepare the model\"\"\"\n","class BaselineModel(Model):\n","    def __init__(self,\n","                 vocab: Vocabulary,\n","                 text_field_embedder: TextFieldEmbedder,\n","                 sentence_encoder: Seq2VecEncoder,\n","                 classifier_feedforward: FeedForward,\n","                 initializer: InitializerApplicator = InitializerApplicator(),\n","                 regularizer: Optional[RegularizerApplicator] = None) -> None:\n","        super(BaselineModel, self).__init__(vocab, regularizer)\n","\n","        self.text_field_embedder = text_field_embedder\n","        self.num_classes = self.vocab.get_vocab_size(\"labels\")\n","        self.sentence_encoder = sentence_encoder\n","        self.classifier_feedforward = classifier_feedforward\n","        self.metrics = {\n","            \"accuracy\": CategoricalAccuracy(),\n","            \"accuracy3\": CategoricalAccuracy(top_k=3)\n","        }\n","        self.loss = torch.nn.CrossEntropyLoss()\n","        initializer(self)\n","\n","    def forward(self,\n","                sentences: Dict[str, torch.LongTensor],\n","                labels: torch.LongTensor = None) -> Dict[str, torch.Tensor]:\n","        embedded_sentence = self.text_field_embedder(sentences)\n","        sentence_mask = util.get_text_field_mask(sentences)\n","        encoded_sentence = self.sentence_encoder(embedded_sentence, sentence_mask)\n","\n","        logits = self.classifier_feedforward(encoded_sentence)\n","        # logits = logits.squeeze(-1) # Added to squeeze 3d to 2d\n","\n","        output_dict = {'logits': logits}\n","        if labels is not None:\n","            print(\"label shape:\", labels.shape)\n","            print(\"logits shape:\", logits.shape)\n","            # loss = self.loss(logits, labels.squeeze(-1))\n","            loss = multiple_target_CrossEntropyLoss(logits, labels)\n","            for metric in self.metrics.values():\n","                metric(logits, labels.squeeze(-1))\n","            output_dict[\"loss\"] = loss\n","\n","        return output_dict\n","\n","    def decode(self, output_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n","        \"\"\"\n","        Coverts tag ids to actual tags.\n","        \"\"\"\n","        for instance_labels in output_dict[\"logits\"]:\n","            print('Instance labels:', instance_labels)\n","        # output_dict[\"labels\"] = [\n","        #     [self.vocab.get_token_from_index(label, namespace='labels')\n","        #          for label in instance_labels]\n","        #         for instance_labels in output_dict[\"logits\"]\n","        # ]\n","        output_dict[\"labels\"] = [\n","            [np.argmax(label.cpu().data.numpy()) for label in instance_labels]\n","                for instance_labels in output_dict[\"logits\"]\n","        ]\n","        return output_dict\n","\n","    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n","        return {metric_name: metric.get_metric(reset) for metric_name, metric in self.metrics.items()}\n"]},{"cell_type":"code","execution_count":190,"metadata":{},"outputs":[],"source":["\"\"\"Prepare embeddings\"\"\"\n","from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n","from allennlp.modules.token_embedders.bert_token_embedder import PretrainedBertEmbedder\n","\n","bert_embedder = PretrainedBertEmbedder(\n","    pretrained_model = \"bert-base-uncased\",\n","    top_layer_only=True,\n","    requires_grad=False\n",")\n","\n","word_embeddings: TextFieldEmbedder = BasicTextFieldEmbedder(\n","                                                            token_embedders={\"tokens\": bert_embedder}, \n","                                                            allow_unmatched_keys=True)\n"]},{"cell_type":"code","execution_count":191,"metadata":{},"outputs":[],"source":["BERT_DIM = word_embeddings.get_output_dim()\n","\n","class BertSentencePooler(Seq2VecEncoder):\n","    def forward(self, embs:torch.tensor, mask:torch.tensor=None) -> torch.tensor:\n","        return embs[:, :, 0]\n","    \n","    def get_output_dim(self) -> int:\n","        return BERT_DIM\n","\n","sentence_encoder = BertSentencePooler(vocab)\n"]},{"cell_type":"code","execution_count":192,"metadata":{},"outputs":[],"source":["classifier_feedforward = nn.Linear(BERT_DIM, 2)\n"]},{"cell_type":"code","execution_count":193,"metadata":{},"outputs":[],"source":["model = BaselineModel(\n","    vocab,\n","    word_embeddings,\n","    sentence_encoder,\n","    classifier_feedforward\n",")\n"]},{"cell_type":"code","execution_count":194,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":"Too many wordpieces, truncating sequence. If you would like a sliding window, set `truncate_long_sequences` to False.The offending input was: ['Utilizing', 'a', 'κ', '-', 'means', 'cluster', 'analysis', 'strategy', ',', 'we', 'demonstrated', 'that', 'one', 'such', 'cluster', 'followed', 'this', 'hypothesized', 'expression', 'pattern', 'over', 'time', ',', 'and', 'that', 'this', 'cluster', 'contained', 'several', 'interrelated', 'transcripts', 'that', 'are', 'classified', 'as', 'regeneration', '-', 'associated', 'genes', '(', 'RAGs', ')', 'including', 'Atf3', ',', 'Sprr1a', ',', 'Ecel1', ',', 'Gadd45a', ',', 'Gpnmb', ',', 'Sox11', ',', 'Mmp19', ',', 'Srgap1', ',', 'Rab15,Lifr', ',', 'Trib3', ',', 'Tgfb1', ',', 'and', 'Sema3c', '.'].To avoid polluting your logs we will not warn about this again.\n"}],"source":["\"\"\"Basic sanity check\"\"\"\n","batch = next(iter(iterator(train_dataset)))\n","tokens = batch[\"sentences\"]\n","labels = batch[\"labels\"]\n"]},{"cell_type":"code","execution_count":195,"metadata":{},"outputs":[],"source":["import allennlp.nn.util as util\n","\n","# mask = util.get_text_field_mask(tokens)\n"]},{"cell_type":"code","execution_count":196,"metadata":{},"outputs":[],"source":["# embeddings = model.text_field_embedder(tokens)\n"]},{"cell_type":"code","execution_count":197,"metadata":{},"outputs":[],"source":["# state = model.sentence_encoder(embeddings, mask)\n"]},{"cell_type":"code","execution_count":198,"metadata":{},"outputs":[],"source":["# logits = model.classifier_feedforward(state)\n","# logits = logits.squeeze(-1)\n"]},{"cell_type":"code","execution_count":199,"metadata":{},"outputs":[],"source":["# loss =  nn.NLLLoss()(logits.reshape(-1, 10), labels.reshape(-1, 10))\n","# def multiple_target_CrossEntropyLoss(logits, labels):\n","#     loss = 0\n","#     for i in range(logits.shape[0]):\n","#         loss = loss + nn.CrossEntropyLoss()(logits[i, :, :], labels[i, :])\n","#     return loss\n"]},{"cell_type":"code","execution_count":200,"metadata":{},"outputs":[],"source":["# loss.backward()\n"]},{"cell_type":"code","execution_count":201,"metadata":{},"outputs":[],"source":["# loss = model(**batch)[\"loss\"]\n"]},{"cell_type":"code","execution_count":207,"metadata":{},"outputs":[],"source":["\"\"\"Train\"\"\"\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","trainer = Trainer(\n","    model=model,\n","    optimizer=optimizer,\n","    iterator=iterator,\n","    validation_iterator=iterator,\n","    train_dataset=train_dataset,\n","    validation_dataset=validation_dataset,\n","    patience=5,\n","    num_epochs=1,\n","    cuda_device=-1\n",")\n"]},{"cell_type":"code","execution_count":208,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":"0%|          | 0/12 [00:00<?, ?it/s]Start training\naccuracy: 0.4498, accuracy3: 1.0000, loss: 49.5887 ||:   8%|▊         | 1/12 [03:00<33:09, 180.88s/it]label shape: torch.Size([64, 10])\nlogits shape: torch.Size([64, 10, 2])\n"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-208-9c36a69c404f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Start training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m~/Documents/Python/Anaconda/anaconda3/envs/allennlp_env/lib/python3.6/site-packages/allennlp/training/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_counter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m             \u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0;31m# get peak of memory usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Documents/Python/Anaconda/anaconda3/envs/allennlp_env/lib/python3.6/site-packages/allennlp/training/trainer.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_group\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfor_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Documents/Python/Anaconda/anaconda3/envs/allennlp_env/lib/python3.6/site-packages/allennlp/training/trainer.py\u001b[0m in \u001b[0;36mbatch_loss\u001b[0;34m(self, batch_group, for_training)\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_group\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_devices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0moutput_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Documents/Python/Anaconda/anaconda3/envs/allennlp_env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-189-ad4f64c4b424>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sentences, labels)\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 labels: torch.LongTensor = None) -> Dict[str, torch.Tensor]:\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0membedded_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_field_embedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0msentence_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_field_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mencoded_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Documents/Python/Anaconda/anaconda3/envs/allennlp_env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Documents/Python/Anaconda/anaconda3/envs/allennlp_env/lib/python3.6/site-packages/allennlp/modules/text_field_embedders/basic_text_field_embedder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text_field_input, num_wrapping_dims, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;31m# is bijective and just use the key directly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext_field_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                 \u001b[0mtoken_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m             \u001b[0membedded_representations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_representations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Documents/Python/Anaconda/anaconda3/envs/allennlp_env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Documents/Python/Anaconda/anaconda3/envs/allennlp_env/lib/python3.6/site-packages/allennlp/modules/token_embedders/bert_token_embedder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, offsets, token_type_ids)\u001b[0m\n\u001b[1;32m    173\u001b[0m         all_encoder_layers, _ = self.bert_model(input_ids=util.combine_initial_dims(input_ids),\n\u001b[1;32m    174\u001b[0m                                                 \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombine_initial_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                                                 attention_mask=util.combine_initial_dims(input_mask))\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0mall_encoder_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_encoder_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Documents/Python/Anaconda/anaconda3/envs/allennlp_env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Documents/Python/Anaconda/anaconda3/envs/allennlp_env/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    731\u001b[0m         encoded_layers = self.encoder(embedding_output,\n\u001b[1;32m    732\u001b[0m                                       \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m                                       output_all_encoded_layers=output_all_encoded_layers)\n\u001b[0m\u001b[1;32m    734\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Documents/Python/Anaconda/anaconda3/envs/allennlp_env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Documents/Python/Anaconda/anaconda3/envs/allennlp_env/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0mall_encoder_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_module\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0mall_encoder_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Documents/Python/Anaconda/anaconda3/envs/allennlp_env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Documents/Python/Anaconda/anaconda3/envs/allennlp_env/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Documents/Python/Anaconda/anaconda3/envs/allennlp_env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Documents/Python/Anaconda/anaconda3/envs/allennlp_env/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Documents/Python/Anaconda/anaconda3/envs/allennlp_env/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mgelu\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mAlso\u001b[0m \u001b[0msee\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0marxiv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1606.08415\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \"\"\"\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["print('Start training')\n","metrics = trainer.train()\n"]},{"cell_type":"code","execution_count":204,"metadata":{},"outputs":[],"source":["\"\"\"Testing\"\"\"\n","\n","class ClaimCrfPredictor(Predictor):\n","    \"\"\"\n","    Predictor wrapper for the AcademicPaperClassifier\n","    \"\"\"\n","    def _json_to_instance(self, json_dict: JsonDict) -> Instance:\n","        sentences = json_dict['sentences']\n","        instance = self._dataset_reader.text_to_instance(sents=sentences)\n","        return instance\n","\n","def read_json(file_path):\n","    \"\"\"\n","    Read list from JSON path\n","    \"\"\"\n","    if not os.path.exists(file_path):\n","        return []\n","    else:\n","        with open(file_path, 'r') as fp:\n","            ls = [json.loads(line) for line in fp]\n","        return ls\n"]},{"cell_type":"code","execution_count":206,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tst Instance with fields:\n \t sentences: ListField of 9 TextFields : \n \t TextField of length 22 with text: \n \t\t[Helical, apolipoproteins, remove, cellular, phospholipid, and, cholesterol, to, generate, nascent,\n\t\tHDL, and, this, reaction, is, the, major, source, of, plasma, HDL, .]\n \t\tand TokenIndexers : {'tokens': 'PretrainedBertIndexer'} \n \t TextField of length 11 with text: \n \t\t[ABCA1, is, mandatory, and, rate, -, limiting, for, this, reaction, .]\n \t\tand TokenIndexers : {'tokens': 'PretrainedBertIndexer'} \n \t TextField of length 43 with text: \n \t\t[Besides, regulation, of, the, gene, expression, by, transcriptional, factors, including, LXR, ,,\n\t\tAP2, and, SREBP, ,, the, ABCA1, activity, is, regulated, post, -, translationally, by, calpain, -,\n\t\tmediated, proteolytic, degradation, of, ABCA1, protein, that, occurs, in, the, early, endosome,\n\t\tafter, its, endocytosis, .]\n \t\tand TokenIndexers : {'tokens': 'PretrainedBertIndexer'} \n \t TextField of length 28 with text: \n \t\t[When, the, HDL, biogenesis, reaction, is, ongoing, as, helical, apolipoproteins, interact, with,\n\t\tABCA1, ,, ABCA1, becomes, resistant, to, calpain, and, is, recycled, to, cell, surface, after,\n\t\tendocytosis, .]\n \t\tand TokenIndexers : {'tokens': 'PretrainedBertIndexer'} \n \t TextField of length 13 with text: \n \t\t[Biogenesis, of, HDL, is, most, likely, to, take, place, on, cell, surface, .]\n \t\tand TokenIndexers : {'tokens': 'PretrainedBertIndexer'} \n \t TextField of length 26 with text: \n \t\t[Clearance, rate, of, ABCA1, by, this, mechanism, is, also, retarded, by, various, factors, that,\n\t\tinteract, with, ABCA1, ,, such, as, α1-syntrophin, ,, LXRβ, and, calmodulin, .]\n \t\tand TokenIndexers : {'tokens': 'PretrainedBertIndexer'} \n \t TextField of length 13 with text: \n \t\t[Physiological, relevance, of, the, retardation, by, these, factors, is, not, entirely, clear, .]\n \t\tand TokenIndexers : {'tokens': 'PretrainedBertIndexer'} \n \t TextField of length 31 with text: \n \t\t[Pharmacological, inhibition, of, the, calpain, -, mediated, ABCA1, degradation, results, in, the,\n\t\tincrease, of, the, ABCA1, activity, and, HDL, biogenesis, in, vitro, and, in, vivo, ,, and,\n\t\tpotentially, suppresses, atherogenesis, .]\n \t\tand TokenIndexers : {'tokens': 'PretrainedBertIndexer'} \n \t TextField of length 30 with text: \n \t\t[This, article, is, part, of, a, Special, Issue, entitled, Advances, in, High, Density, Lipoprotein,\n\t\tFormation, and, Metabolism, :, A, Tribute, to, John, F., Oram, (, 1945, -, 2010, ), .]\n \t\tand TokenIndexers : {'tokens': 'PretrainedBertIndexer'} \n \n \t labels: SequenceLabelField of length 9 with labels:\n \t\t['0', '0', '0', '0', '0', '0', '0', '1', '0']\n \t\tin namespace: 'labels'. \n\nlabel shape: torch.Size([1, 9])\nlogits shape: torch.Size([1, 9, 2])\nInstance labels: tensor([[-0.0936, -0.0218],\n        [-0.0131, -0.0102],\n        [ 0.0286,  0.0102],\n        [ 0.1144, -0.2205],\n        [ 0.1511, -0.1736],\n        [ 0.1525, -0.2279],\n        [ 0.1200, -0.1060],\n        [ 0.0281,  0.0888],\n        [ 0.1342, -0.1208]])\nPred output: {'logits': [[-0.09358204901218414, -0.021841466426849365], [-0.013108652085065842, -0.010208098217844963], [0.028617747128009796, 0.010157037526369095], [0.11444569379091263, -0.2204647660255432], [0.15114718675613403, -0.1736261546611786], [0.15247783064842224, -0.22794345021247864], [0.11996693164110184, -0.10602877289056778], [0.028065599501132965, 0.08879713714122772], [0.1341557502746582, -0.12080297619104385]], 'loss': 0.6151340007781982, 'labels': [1, 1, 0, 0, 0, 0, 0, 1, 0]}\ntorch.Size([9, 2])\nLogits output: tensor([[-0.0936, -0.0218],\n        [-0.0131, -0.0102],\n        [ 0.0286,  0.0102],\n        [ 0.1144, -0.2205],\n        [ 0.1511, -0.1736],\n        [ 0.1525, -0.2279],\n        [ 0.1200, -0.1060],\n        [ 0.0281,  0.0888],\n        [ 0.1342, -0.1208]])\nY true: [0 0 0 0 0 0 0 1 0]\nY pred: [1 1 0 0 0 0 0 1 0]\nTest score: (0.3333333333333333, 1.0, 0.5, None)\n"}],"source":["test_list = read_json(cached_path(TEST_PATH))\n","claim_predictor = ClaimCrfPredictor(model, dataset_reader=reader)\n","y_pred, y_true = [], []\n","for tst in test_dataset:\n","    print('tst', tst)\n","    pred = claim_predictor.predict_instance(tst)\n","    print('Pred output:', pred)\n","    logits = torch.FloatTensor(pred['logits'])\n","    print(logits.shape)\n","    print('Logits output:', logits)\n","#     best_paths = model.crf.viterbi_tags(torch.FloatTensor(pred['logits']).unsqueeze(0), \n","#                                         torch.LongTensor(pred['mask']).unsqueeze(0))\n","    predicted_labels = pred['labels']\n","    y_pred.extend(predicted_labels)\n","    y_true.extend(tst['labels'])\n","    break\n","y_true = np.array(y_true).astype(int)\n","y_pred = np.array(y_pred).astype(int)\n","print('Y true:', y_true)\n","print('Y pred:', y_pred)\n","print('Test score:', precision_recall_fscore_support(y_true, y_pred, average='binary'))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.9"},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}